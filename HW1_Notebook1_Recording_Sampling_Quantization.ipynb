{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d361ec8",
   "metadata": {},
   "source": [
    "# EE 519 — Speech AI\n",
    "## HW-1 | Notebook 1: Recording, Sampling & Quantization\n",
    "\n",
    "**Student Name:**  \n",
    "**USC ID:**  \n",
    "**Date:**  \n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "By completing this notebook, you will:\n",
    "- Understand the transition from continuous speech to digital signals\n",
    "- Explore the effects of sampling rate and quantization\n",
    "- Connect visual distortions to perceptual changes\n",
    "- Develop intuition for minimum requirements in speech processing\n",
    "\n",
    "> ⚠️ **Important**\n",
    "> - All answers (code + explanations) must be written **inside this notebook**\n",
    "> - Do **not** delete questions or prompts\n",
    "> - Clearly label all plots (title, axes, units)\n",
    "> - Audio must be playable inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaa6b15",
   "metadata": {},
   "source": [
    "### Grading (Notebook 1 — 20 points)\n",
    "\n",
    "| Component | Points |\n",
    "|---|---:|\n",
    "| Correct signal recording & handling | 4 |\n",
    "| Sampling experiments & plots | 5 |\n",
    "| Quantization experiments & plots | 5 |\n",
    "| Observations & explanations | 4 |\n",
    "| Clarity & organization | 2 |\n",
    "\n",
    "> We grade **understanding and reasoning**, not perfection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19897a36",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 0. Setup\n",
    "\n",
    "This notebook is designed to work with **your own recordings**. You will record:\n",
    "1. A sustained vowel (≈3 seconds), e.g., `/a/`, `/i/`, or `/u/`\n",
    "2. A spoken sentence (≈3–6 seconds), e.g., **“I am taking a speech AI course.”**\n",
    "\n",
    "## File requirements\n",
    "- WAV format recommended\n",
    "- Mono (one channel) preferred\n",
    "- Keep filenames simple (no spaces), e.g.:\n",
    "  - `vowel.wav`\n",
    "  - `sentence.wav`\n",
    "\n",
    "Place your files in the same folder as this notebook, or update the path variables below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load required libraries\n",
    "# Recommended: numpy, scipy, matplotlib\n",
    "# Optional (helpful): soundfile, librosa, IPython.display\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# If you want inline audio playback:\n",
    "from IPython.display import Audio, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a0e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set paths to your recordings\n",
    "VOWEL_PATH = \"vowel.wav\"\n",
    "SENTENCE_PATH = \"sentence.wav\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a robust WAV loader.\n",
    "# Requirements:\n",
    "# - Return signal as float32 in [-1, 1]\n",
    "# - Return sample rate fs\n",
    "# - If stereo, convert to mono (e.g., average channels)\n",
    "\n",
    "# Hints:\n",
    "# - You can use scipy.io.wavfile.read OR soundfile.read\n",
    "# - Be careful with integer PCM formats (int16/int32)\n",
    "\n",
    "def load_wav(path):\n",
    "    raise NotImplementedError(\"Implement load_wav(path)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load your recordings\n",
    "# 1) vowel signal\n",
    "# 2) sentence signal\n",
    "\n",
    "# x_vowel, fs_vowel = load_wav(VOWEL_PATH)\n",
    "# x_sentence, fs_sentence = load_wav(SENTENCE_PATH)\n",
    "\n",
    "# Print:\n",
    "# - sampling rate\n",
    "# - duration (seconds)\n",
    "# - min/max amplitude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee5e36a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Recording Quality: Visualization & Playback\n",
    "\n",
    "### Task\n",
    "For each signal (vowel + sentence):\n",
    "1. Plot the **full waveform**\n",
    "2. Plot a **zoomed-in** segment (50–100 ms)\n",
    "3. Play the audio inline and confirm:\n",
    "   - No audible clipping\n",
    "   - Reasonable loudness\n",
    "   - Signal is not truncated\n",
    "\n",
    "### Notes\n",
    "- Use consistent labeling: time in seconds on x-axis, amplitude on y-axis.\n",
    "- For zoomed segment, pick a region that is not silence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c555aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement helper plotting utilities\n",
    "# Example suggestions:\n",
    "# - plot_waveform(x, fs, title, tlim=None) where tlim is (t_start, t_end) in seconds\n",
    "# - select_zoom_region(x, fs, start_sec, duration_ms)\n",
    "\n",
    "def plot_waveform(x, fs, title, tlim=None):\n",
    "    raise NotImplementedError(\"Implement plot_waveform\")\n",
    "\n",
    "def play_audio(x, fs):\n",
    "    # This is optional but recommended\n",
    "    display(Audio(x, rate=fs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6db81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot and play vowel\n",
    "# plot_waveform(x_vowel, fs_vowel, \"Vowel: full waveform\")\n",
    "# plot_waveform(x_vowel, fs_vowel, \"Vowel: zoomed waveform\", tlim=(..., ...))\n",
    "# play_audio(x_vowel, fs_vowel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot and play sentence\n",
    "# plot_waveform(x_sentence, fs_sentence, \"Sentence: full waveform\")\n",
    "# plot_waveform(x_sentence, fs_sentence, \"Sentence: zoomed waveform\", tlim=(..., ...))\n",
    "# play_audio(x_sentence, fs_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d75b7fd",
   "metadata": {},
   "source": [
    "### Observations (Recording Quality)\n",
    "\n",
    "Answer briefly (3–6 bullets total):\n",
    "\n",
    "- Is the waveform symmetric around 0? What might cause asymmetry?\n",
    "- Are there visible silent regions? Where?\n",
    "- Do you see any evidence of clipping (flat tops / bottoms)?\n",
    "- Compare vowel vs sentence: what differences do you observe in amplitude dynamics?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71aa097",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Sampling Rate Experiments (Perceptual + Visual)\n",
    "\n",
    "You will analyze how **sampling frequency** affects speech. Create resampled versions of **one** chosen signal first (recommended: the sentence), then optionally repeat for the vowel.\n",
    "\n",
    "### Required target sampling rates\n",
    "- Original sampling rate (reference)\n",
    "- 16 kHz\n",
    "- 8 kHz\n",
    "- 4 kHz\n",
    "\n",
    "> Use **proper resampling** (anti-aliasing filter + resample), not naive decimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2564eca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose which signal to use for sampling experiments\n",
    "# Suggested: sentence\n",
    "x_ref = None   # e.g., x_sentence\n",
    "fs_ref = None  # e.g., fs_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ccd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement resampling utility\n",
    "# Requirements:\n",
    "# - Accept x, fs_in, fs_out\n",
    "# - Return x_out (float in [-1, 1]) and fs_out\n",
    "\n",
    "def resample_signal(x, fs_in, fs_out):\n",
    "    raise NotImplementedError(\"Implement resample_signal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create resampled versions at 16k, 8k, 4k\n",
    "# Store them in a dictionary for convenience.\n",
    "\n",
    "target_fs_list = [16000, 8000, 4000]\n",
    "signals = {}  # e.g., {\"orig\": (x_ref, fs_ref), \"16k\": (...), ...}\n",
    "\n",
    "# signals[\"orig\"] = (x_ref, fs_ref)\n",
    "# for fs_tgt in target_fs_list:\n",
    "#     x_tgt = resample_signal(x_ref, fs_ref, fs_tgt)\n",
    "#     signals[f\"{fs_tgt//1000}k\"] = (x_tgt, fs_tgt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da5fb37",
   "metadata": {},
   "source": [
    "## 2.1 Time-domain comparison\n",
    "\n",
    "For each sampling rate:\n",
    "- Plot the **same time segment** (same start time and duration in seconds)\n",
    "- Use the **same y-limits** across plots if possible (helps comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6db889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the same segment for each sampling rate\n",
    "# Choose a segment with speech activity (not silence).\n",
    "# Example:\n",
    "# segment_start = 0.5  # seconds\n",
    "# segment_dur = 0.2    # seconds\n",
    "# for key, (x, fs) in signals.items():\n",
    "#     plot_waveform(x, fs, f\"{key}: waveform segment\", tlim=(segment_start, segment_start + segment_dur))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961cbdc5",
   "metadata": {},
   "source": [
    "## 2.2 Frequency-domain comparison\n",
    "\n",
    "For each sampling rate:\n",
    "- Compute magnitude spectrum (FFT) for the **same segment length in seconds**\n",
    "- Plot magnitude vs frequency (Hz)\n",
    "- Use consistent plotting choices so the comparison is meaningful\n",
    "\n",
    "### Hint (recommended approach)\n",
    "- Apply a window (e.g., Hann) before FFT\n",
    "- Plot only up to Nyquist (fs/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecacec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement FFT magnitude spectrum helper\n",
    "def magnitude_spectrum(x, fs):\n",
    "    \"\"\"Return frequency axis (Hz) and magnitude spectrum (linear or dB).\"\"\"\n",
    "    raise NotImplementedError(\"Implement magnitude_spectrum\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0613749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot spectra for each sampling rate\n",
    "# for key, (x, fs) in signals.items():\n",
    "#     f, mag = magnitude_spectrum(...)\n",
    "#     plt.figure()\n",
    "#     plt.plot(f, mag)\n",
    "#     plt.title(...)\n",
    "#     plt.xlabel(\"Frequency (Hz)\")\n",
    "#     plt.ylabel(\"Magnitude (dB)\" or \"Magnitude\")\n",
    "#     plt.xlim(0, fs/2)\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d789e90",
   "metadata": {},
   "source": [
    "### Observations (Sampling)\n",
    "\n",
    "Answer in 6–10 lines total:\n",
    "\n",
    "- At what sampling rate does intelligibility begin to degrade (for your chosen signal)?\n",
    "- Where do you visually observe aliasing (if at all)? What plot made it obvious?\n",
    "- Does aliasing appear before or after speech becomes unintelligible?\n",
    "- Does the vowel degrade differently than the sentence? Why might that be?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbec684",
   "metadata": {},
   "source": [
    "### Conceptual Reasoning (Sampling)\n",
    "\n",
    "Answer clearly in complete sentences:\n",
    "\n",
    "1. What is the **minimum sampling rate** you would recommend for intelligible speech?  \n",
    "2. Does the Nyquist criterion alone guarantee **perceptual** quality? Explain.  \n",
    "3. Why does speech remain intelligible even when high frequencies are lost? (Connect to speech cues.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd79272",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Quantization Experiments\n",
    "\n",
    "You will study **bit depth** and its perceptual impact.\n",
    "\n",
    "### Task\n",
    "Quantize the *same reference signal* to:\n",
    "- 16-bit (reference)\n",
    "- 8-bit\n",
    "- 4-bit\n",
    "- 2-bit\n",
    "\n",
    "> Implement **uniform quantization** yourself (do not call a pre-made quantizer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement uniform quantization\n",
    "# Requirements:\n",
    "# - Input: x in [-1, 1]\n",
    "# - Parameter: n_bits (e.g., 16, 8, 4, 2)\n",
    "# - Output: quantized signal x_q in [-1, 1]\n",
    "# - Also return quantization step size Δ (optional but recommended)\n",
    "\n",
    "def uniform_quantize(x, n_bits):\n",
    "    raise NotImplementedError(\"Implement uniform_quantize\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964eeb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate quantized versions\n",
    "# q_versions = {}\n",
    "# for n_bits in [16, 8, 4, 2]:\n",
    "#     x_q = uniform_quantize(x_ref, n_bits)\n",
    "#     q_versions[f\"{n_bits}bit\"] = x_q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa9b950",
   "metadata": {},
   "source": [
    "## 3.1 Quantized waveforms (zoom)\n",
    "\n",
    "- Plot original vs quantized for a **50–100 ms** segment\n",
    "- Overlay plots or show separate plots (your choice)\n",
    "- Label clearly with bit depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0171e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot original vs quantized signals (zoomed)\n",
    "# tlim = ( ..., ... )\n",
    "# plot_waveform(x_ref, fs_ref, \"Original (reference)\", tlim=tlim)\n",
    "# for key, x_q in q_versions.items():\n",
    "#     plot_waveform(x_q, fs_ref, f\"Quantized: {key}\", tlim=tlim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5d13e",
   "metadata": {},
   "source": [
    "## 3.2 Quantization error\n",
    "\n",
    "Compute and visualize quantization error:\n",
    "- error = original - quantized\n",
    "- Plot error for a short segment\n",
    "- (Optional) plot histogram of error values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cffaf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute and plot quantization error\n",
    "# for key, x_q in q_versions.items():\n",
    "#     err = x_ref - x_q\n",
    "#     ... plot err ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6426345e",
   "metadata": {},
   "source": [
    "## 3.3 Listening test\n",
    "\n",
    "For each quantized version:\n",
    "- Play audio inline\n",
    "- Write 1–2 lines describing what you hear\n",
    "\n",
    "> Tip: Use headphones for low-bit-depth signals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40470261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Play quantized audio versions\n",
    "# for key, x_q in q_versions.items():\n",
    "#     print(key)\n",
    "#     play_audio(x_q, fs_ref)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22f50d1",
   "metadata": {},
   "source": [
    "### Observations (Quantization)\n",
    "\n",
    "Answer in 6–10 lines total:\n",
    "\n",
    "- Which bit depth introduces clearly audible distortion first?\n",
    "- Is distortion more noticeable in vowels or fricatives? Why?\n",
    "- Does increasing sampling rate reduce quantization artifacts? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f4bd20",
   "metadata": {},
   "source": [
    "### Conceptual Reasoning (Quantization)\n",
    "\n",
    "Answer clearly:\n",
    "\n",
    "1. Why does quantization noise sound different from additive white noise?  \n",
    "2. Why is speech surprisingly robust to low bit depth (to a point)?  \n",
    "3. How is quantization related to dynamic range and SNR?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3368769",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Sampling vs Quantization — Combined Effects\n",
    "\n",
    "Compare the following two degradation types:\n",
    "\n",
    "1. **High sampling + low bit depth** (e.g., 16 kHz at 2-bit or 4-bit)  \n",
    "2. **Low sampling + high bit depth** (e.g., 4 kHz at 16-bit)\n",
    "\n",
    "### Task\n",
    "Create two or more paired conditions and evaluate:\n",
    "- Waveform (zoom)\n",
    "- Spectrum\n",
    "- Listening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92fdd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build combined conditions (choose at least two comparisons)\n",
    "# Example conditions:\n",
    "# - (16k, 4-bit) vs (4k, 16-bit)\n",
    "# - (16k, 2-bit) vs (8k, 16-bit)\n",
    "#\n",
    "# You may need to:\n",
    "# - resample first, then quantize (or vice versa)\n",
    "# - be consistent and document your pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26365b4",
   "metadata": {},
   "source": [
    "### Observations (Combined Effects)\n",
    "\n",
    "Answer in 6–10 lines total:\n",
    "\n",
    "- Which degradation is more perceptually damaging for your signal?\n",
    "- Under what conditions does quantization dominate?\n",
    "- Under what conditions does sampling dominate?\n",
    "- Give one example from your plots that supports your conclusion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1cd592",
   "metadata": {},
   "source": [
    "### Final Conceptual Insight (Combined)\n",
    "\n",
    "Explain (8–12 lines):\n",
    "\n",
    "Why do **sampling frequency** and **quantization resolution** affect *different perceptual dimensions* of speech?\n",
    "\n",
    "Use your own experiment results as evidence (point to specific plots or conditions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a854e31a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Reflection (Mandatory)\n",
    "\n",
    "Write thoughtful answers. There are no “right” responses here—only honest reflection.\n",
    "\n",
    "### Reflection prompts\n",
    "1. **What did you learn** that you did not understand before doing the experiments?  \n",
    "2. What was a **sudden surprise / unexpected result** you noticed (visual or perceptual)?  \n",
    "3. What was one moment where you thought you understood something, but the plots/audio **challenged your intuition**?  \n",
    "4. If you had to explain sampling vs quantization to a friend in **two sentences**, what would you say?  \n",
    "5. What is one question you now have that you want to explore further?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404446c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. AI Use Disclosure (Required)\n",
    "\n",
    "If you used any AI tools (including ChatGPT), briefly describe:\n",
    "- What you used it for (e.g., debugging, concept clarification)\n",
    "- What you wrote/changed yourself\n",
    "\n",
    "Example:\n",
    "- “Used ChatGPT to understand how resampling works conceptually; wrote resampling code myself.”\n",
    "\n",
    "*(If you did not use AI, write “No AI tools used.”)*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
